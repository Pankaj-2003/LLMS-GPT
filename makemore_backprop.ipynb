{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/PliMWisvN+5BJHSPXUr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pankaj-2003/LLMS-GPT/blob/main/makemore_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boilerplate"
      ],
      "metadata": {
        "id": "Chjj-8_LnFOE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uyMV58xQbGs-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbDTTGtXm2GY",
        "outputId": "26035c9a-0e26-487f-de38-557d8c732692"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwlv3Uqpm8TP",
        "outputId": "47dbb655-1589-4158-fc1f-ae971b89a40f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbAcv0nvm8hI",
        "outputId": "dd87e388-07dd-4d72-b17b-09e2821a7712"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGcp0pzmnA6p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "-NATGDnC3rmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e2f49d-9dcd-4892-cac3-9f4d2c9b0e50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDz7tDbgj8Dn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ],
      "metadata": {
        "id": "Lb4127L_YYWu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TcCwYy21l1EY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5  # normalize the data\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvNOns_WYaFn",
        "outputId": "4d61ae65-8f8a-4e1c-94ae-64bc66b9179d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3354, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsQBZBYn-tng"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = -(a+b+c)/3\n",
        "# loss = -1/3a + -1/3b + -1/3c\n",
        "# dloss/da = -1/n\n",
        "# dloss/db = -1/n\n",
        "# dloss/dc = -1/n"
      ],
      "metadata": {
        "id": "_iLSDgqPoqx4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dlogprobs = torch.zeros_like(logprobs)  #dloss/dlogprobs\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "dprobs = (1/probs) * dlogprobs             # dloss/dprobs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1 , keepdim = True) # dloss/dcounts_sum_inv\n",
        "dcounts_sum = (-(counts_sum)**-2) * dcounts_sum_inv # dloss/dcounts_sum\n",
        "dcounts = (1 * dcounts_sum) + (counts_sum_inv * dprobs) # dloss/dcounts\n",
        "# dcounts = torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits =  norm_logits.exp() * dcounts             #dloss/dnorm_logits\n",
        "dlogit_maxes = -dnorm_logits          #dloss/dlogit_maxes\n",
        "max_mask = torch.zeros_like(logits)\n",
        "max_indices = logits.argmax(dim = 1)\n",
        "max_mask = logits[torch.arange()]\n",
        "dlogits = dnorm_logits * 1 + dlogit_maxes *\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "# cmp('h', dh, h)\n",
        "# cmp('W2', dW2, W2)\n",
        "# cmp('b2', db2, b2)\n",
        "# cmp('hpreact', dhpreact, hpreact)\n",
        "# cmp('bngain', dbngain, bngain)\n",
        "# cmp('bnbias', dbnbias, bnbias)\n",
        "# cmp('bnraw', dbnraw, bnraw)\n",
        "# cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "# cmp('bnvar', dbnvar, bnvar)\n",
        "# cmp('bndiff2', dbndiff2, bndiff2)\n",
        "# cmp('bndiff', dbndiff, bndiff)\n",
        "# cmp('bnmeani', dbnmeani, bnmeani)\n",
        "# cmp('hprebn', dhprebn, hprebn)\n",
        "# cmp('embcat', dembcat, embcat)\n",
        "# cmp('W1', dW1, W1)\n",
        "# cmp('b1', db1, b1)\n",
        "# cmp('emb', demb, emb)\n",
        "# cmp('C', dC, C)"
      ],
      "metadata": {
        "id": "rFTd7FQkYbrA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b0fb232d-2f90-4871-87e8-a41733a52f1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-10-7ad9d2b33b03>, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-7ad9d2b33b03>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    dlogits = dlogit_maxes *\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IpU1jTt7ztT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9804c410-0fbe-4aa6-f1e4-66e8fcbf2690"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5,  2, 19, 15, 15, 25, 16,  3, 19,  8, 15,  3, 22,  5,  7,  5,  2,  1,\n",
              "        22, 19, 15, 19, 22, 22, 23,  5, 22, 20, 16,  8, 24, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HfvbinGkiHvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c3e3a1-f957-48a7-ed73-02378868a84e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.0305e-01,  1.0414e+00, -5.0716e-01,  4.7363e-01, -4.6248e-01,\n",
              "          1.0565e+00, -3.1924e-01,  1.4160e-01, -5.4367e-01, -3.0904e-02,\n",
              "          1.3783e-01,  2.1669e-01,  1.4790e-01, -1.1118e-01,  9.1599e-02,\n",
              "         -8.9058e-01, -1.2488e+00, -5.5213e-01, -7.3097e-01,  5.3804e-01,\n",
              "          4.4107e-01, -3.5259e-01, -1.3948e-01,  7.3583e-01,  5.8775e-01,\n",
              "         -1.5147e-01, -4.1530e-01],\n",
              "        [ 3.8006e-01,  3.2385e-01,  8.7071e-01,  4.0742e-01, -1.1783e-01,\n",
              "          1.7906e-02, -7.7828e-01,  1.4021e-01, -6.2908e-01, -5.3790e-01,\n",
              "          1.7208e-01,  1.7767e-01,  1.4848e-01, -3.0070e-01,  5.5500e-02,\n",
              "         -1.1047e-01, -3.2747e-01, -8.9233e-01, -6.6318e-01,  3.9258e-02,\n",
              "         -7.4389e-01, -4.9892e-01, -9.3843e-01,  4.0269e-01, -5.3003e-01,\n",
              "         -4.3948e-02, -4.7365e-01],\n",
              "        [-4.4907e-01, -3.1554e-01, -7.6571e-01, -9.3893e-01, -4.6034e-01,\n",
              "          3.2157e-01,  6.0793e-01,  6.5798e-01,  6.4026e-01, -1.2881e-01,\n",
              "         -4.8370e-01,  1.0199e-01,  1.9217e-01,  3.3631e-01, -4.9046e-01,\n",
              "         -2.9445e-01, -8.3451e-01, -4.3885e-02, -1.3991e-01,  1.1681e+00,\n",
              "          6.3036e-01,  1.2871e-01,  2.3803e-01,  1.8747e-01,  1.1912e-01,\n",
              "         -4.7867e-01, -1.6519e-01],\n",
              "        [-2.8016e-02, -2.7918e-01,  2.4412e-01,  3.7885e-01,  4.4460e-01,\n",
              "         -1.1288e-01,  2.7256e-01,  1.8169e-01,  4.1561e-01, -7.0671e-01,\n",
              "          3.2974e-02, -2.9697e-01, -7.8701e-02,  1.2456e-01,  5.0937e-01,\n",
              "          5.7615e-01, -3.1562e-01, -3.3286e-01, -7.0227e-01,  4.2284e-01,\n",
              "         -7.1461e-01, -3.4747e-01,  7.9009e-02, -5.7173e-01, -3.0918e-01,\n",
              "          2.8250e-01,  2.6375e-01],\n",
              "        [-6.4817e-01, -5.5728e-01, -2.5775e-01, -3.8524e-01, -7.5746e-01,\n",
              "          4.0006e-01,  2.5716e-01, -5.9391e-01,  6.0086e-01,  1.1791e-01,\n",
              "         -1.6220e-02, -6.6156e-01,  1.2175e-01, -3.3747e-01,  2.8062e-01,\n",
              "          1.6248e+00, -5.0937e-01,  4.4998e-01,  4.5493e-01,  7.8453e-01,\n",
              "          2.1114e-01, -6.6156e-01, -3.8409e-01, -8.0766e-01,  3.4552e-01,\n",
              "         -2.9334e-01,  3.7380e-01],\n",
              "        [ 2.4598e-01,  1.2294e-01,  4.7000e-01,  7.0813e-01,  6.6075e-01,\n",
              "         -9.4577e-02, -7.9179e-02,  5.3327e-01, -1.5829e-01, -7.7395e-01,\n",
              "          3.3423e-01, -5.0355e-01, -2.0308e-01,  2.4175e-01,  6.7725e-01,\n",
              "          3.7351e-01, -6.9882e-01, -4.4176e-01, -1.1300e+00, -7.0298e-02,\n",
              "         -6.2968e-02, -3.8457e-02,  2.9233e-01, -5.9562e-01, -6.6847e-01,\n",
              "          9.0359e-01,  3.6223e-01],\n",
              "        [ 5.0695e-01, -1.1246e+00,  4.9772e-01, -9.2336e-01, -3.2230e-01,\n",
              "         -3.8816e-01,  5.6148e-01,  4.3457e-01,  3.1369e-01,  6.7642e-02,\n",
              "         -7.9682e-01, -3.1156e-02, -3.8756e-01, -7.9195e-01, -9.4827e-01,\n",
              "          5.4347e-01,  7.6727e-01,  1.8837e-01, -1.2354e+00,  4.8764e-02,\n",
              "         -3.0332e-01,  3.6725e-01,  2.1129e-01, -5.8536e-01,  5.2819e-02,\n",
              "         -4.2360e-01, -1.2069e-01],\n",
              "        [ 5.0628e-01,  1.0527e-01, -8.1074e-02,  1.4184e+00, -5.5983e-02,\n",
              "          6.9434e-01,  7.2213e-02,  1.1199e+00,  3.2214e-01, -1.2215e+00,\n",
              "         -2.6371e-02, -5.6703e-01, -4.0270e-01,  7.1322e-01, -5.0021e-01,\n",
              "         -6.5580e-01, -7.4563e-01, -7.6585e-01, -6.3268e-01,  1.1446e+00,\n",
              "          2.0255e-01,  5.6147e-01, -8.9576e-02,  2.3250e-01, -2.0269e-01,\n",
              "          7.2413e-01,  2.2103e-01],\n",
              "        [ 4.9297e-01, -1.3479e-01, -5.6177e-01,  2.2427e-01, -1.0825e+00,\n",
              "          3.8932e-01,  5.8402e-01,  1.0920e+00, -7.5157e-02, -4.3642e-01,\n",
              "         -1.0337e-01,  7.8657e-02,  1.4182e-01, -2.6329e-01, -1.0220e+00,\n",
              "         -5.1785e-01, -4.3388e-01,  1.3626e-01, -4.0358e-01,  1.1331e+00,\n",
              "          1.4507e-01,  1.0628e+00,  3.2372e-01,  2.9751e-01,  3.4941e-01,\n",
              "         -6.3199e-01,  1.6028e-01],\n",
              "        [-1.1702e-02, -7.0263e-01, -1.6871e-01, -5.7737e-01, -5.0030e-02,\n",
              "          3.1186e-01,  2.0129e-02,  3.8473e-03,  1.1042e+00,  6.0296e-01,\n",
              "         -8.6062e-01,  1.6973e-01,  1.4353e-01,  2.2581e-01, -3.4884e-01,\n",
              "         -2.2558e-01,  6.9102e-01,  2.8828e-01,  3.0449e-01, -2.1326e-01,\n",
              "          7.4574e-01, -1.9902e-01,  7.2918e-01, -5.2785e-01, -3.3465e-02,\n",
              "          1.8987e-01, -4.6351e-01],\n",
              "        [-4.6595e-01, -1.1751e+00,  3.7501e-01,  5.4556e-01,  1.6156e-01,\n",
              "          4.0880e-01, -3.1311e-01, -1.3530e+00,  3.8875e-01,  4.5116e-01,\n",
              "         -2.4697e-02, -2.6277e-01,  7.0308e-01,  1.1064e-01,  3.4631e-01,\n",
              "          1.6859e+00,  7.7910e-01,  7.0197e-01,  1.1056e+00,  3.3111e-01,\n",
              "          4.6649e-01, -9.0821e-01, -7.6592e-01, -1.9332e-01,  3.3417e-01,\n",
              "          2.9248e-02, -3.9301e-02],\n",
              "        [-1.5852e-02,  1.7242e-01, -1.8831e-01,  2.0357e+00, -7.3555e-01,\n",
              "          9.5881e-01,  1.0523e+00,  5.8372e-01,  5.4090e-01,  1.6573e-01,\n",
              "         -4.8405e-01, -2.3519e-01, -2.5099e-01,  7.0691e-03, -1.3500e+00,\n",
              "         -1.3545e+00, -1.5417e-01,  2.6620e-01, -5.1169e-01,  8.2112e-01,\n",
              "          6.8741e-01,  4.4493e-01, -3.3206e-01,  3.4558e-01,  5.6624e-01,\n",
              "          5.8860e-01, -3.8066e-01],\n",
              "        [-5.0464e-01, -7.8613e-01,  3.5075e-01, -8.2175e-01, -2.6931e-01,\n",
              "         -5.7837e-01,  9.7322e-01,  2.0727e-01,  6.9966e-01,  4.0190e-01,\n",
              "         -7.2755e-01,  1.8892e-01,  2.0150e-01, -1.1822e-01, -8.1386e-01,\n",
              "          1.5632e-01,  8.3550e-01,  2.9449e-01,  2.4464e-01, -2.4138e-01,\n",
              "          3.5047e-01,  2.2140e-01,  1.0918e+00, -1.0935e+00, -3.7177e-01,\n",
              "         -1.8837e-01, -1.1280e-01],\n",
              "        [-3.5461e-01, -1.7889e-01,  5.2849e-01, -5.0188e-01, -7.5198e-01,\n",
              "          8.9474e-01, -1.7375e-01, -7.6998e-01, -5.7619e-02,  2.6536e-01,\n",
              "         -2.1938e-01,  5.3876e-02,  6.2484e-01,  1.1378e-01,  3.0483e-01,\n",
              "          2.3890e-01, -9.8497e-02, -8.4587e-02,  8.3184e-01, -7.4492e-01,\n",
              "          5.5725e-01, -7.6337e-01, -6.3785e-01, -6.2584e-02,  6.7684e-03,\n",
              "          2.2732e-01,  1.1939e-01],\n",
              "        [ 3.9484e-01, -3.5159e-01, -1.3841e-01,  5.3307e-01, -6.2574e-01,\n",
              "          1.4798e-01, -1.1950e-01,  5.5439e-01,  5.5259e-01,  3.3329e-01,\n",
              "         -1.2041e-01,  3.3175e-01,  3.5280e-01,  2.0192e-01, -2.0983e-01,\n",
              "         -3.4637e-01, -3.6931e-01,  4.3097e-01, -2.5969e-01,  4.0424e-01,\n",
              "          4.0943e-01,  4.0286e-01, -2.5299e-01,  2.9243e-02, -4.7093e-01,\n",
              "         -4.2550e-01, -3.3683e-01],\n",
              "        [ 2.0930e-01,  4.9554e-01, -1.6137e-01, -6.6445e-01, -6.8592e-01,\n",
              "          8.6287e-01, -3.2709e-01, -4.0337e-01, -1.7549e-01,  2.6349e-01,\n",
              "         -1.0990e-01,  2.9412e-01,  4.2295e-01, -7.5415e-01, -3.4098e-01,\n",
              "         -9.0874e-01,  1.1542e-01, -7.1087e-01,  4.0022e-01, -6.4363e-01,\n",
              "          3.0089e-02, -3.7863e-01, -1.2688e-01,  5.7869e-01,  2.8937e-01,\n",
              "         -1.9976e-01, -5.3964e-01],\n",
              "        [-4.5602e-02, -2.7335e-01,  9.9192e-01,  4.2187e-01,  5.7130e-01,\n",
              "          5.9559e-01, -6.5439e-01, -1.6301e-01, -4.4832e-01, -7.0143e-01,\n",
              "          4.3786e-01, -3.4141e-01,  1.8012e-01,  4.8887e-01,  6.6944e-01,\n",
              "          6.0998e-01, -7.0534e-01, -6.9994e-01, -3.6344e-01,  3.9261e-01,\n",
              "         -2.1952e-01, -6.3277e-01, -1.2994e+00,  3.0271e-01, -4.0733e-01,\n",
              "          2.8037e-01,  5.4683e-01],\n",
              "        [ 3.8401e-01,  7.7852e-01,  3.3622e-01,  4.4086e-01,  4.3018e-02,\n",
              "          4.2826e-03, -4.9846e-01,  7.3202e-01, -3.0063e-01, -7.6572e-01,\n",
              "          5.5564e-01, -8.6091e-01, -4.4809e-01, -6.2949e-01,  1.1195e-01,\n",
              "         -3.1720e-01, -8.9251e-01, -1.8313e-01, -7.9713e-01,  4.8545e-01,\n",
              "         -5.7070e-01,  2.8748e-01, -3.1789e-01, -6.4473e-01, -4.0378e-01,\n",
              "          1.8247e-01,  5.2088e-01],\n",
              "        [-5.0464e-01, -7.8613e-01,  3.5075e-01, -8.2175e-01, -2.6931e-01,\n",
              "         -5.7837e-01,  9.7322e-01,  2.0727e-01,  6.9966e-01,  4.0190e-01,\n",
              "         -7.2755e-01,  1.8892e-01,  2.0150e-01, -1.1822e-01, -8.1386e-01,\n",
              "          1.5632e-01,  8.3550e-01,  2.9449e-01,  2.4464e-01, -2.4138e-01,\n",
              "          3.5047e-01,  2.2140e-01,  1.0918e+00, -1.0935e+00, -3.7177e-01,\n",
              "         -1.8837e-01, -1.1280e-01],\n",
              "        [-3.4328e-01, -4.9668e-01, -3.1518e-02, -8.4807e-01, -9.6058e-02,\n",
              "          3.5658e-02,  6.7329e-01,  3.1330e-01, -5.4532e-03, -1.3593e-01,\n",
              "         -2.7573e-01,  3.0280e-01,  4.0661e-01,  3.4589e-01, -1.0846e+00,\n",
              "         -5.0094e-01, -1.9534e-01, -6.9096e-01, -2.1363e-01,  9.0444e-01,\n",
              "          2.7510e-01, -5.8123e-02,  1.4570e-01,  2.3965e-01, -2.1414e-01,\n",
              "         -4.6060e-01, -3.6745e-01],\n",
              "        [ 5.6625e-01, -8.3130e-01, -6.0715e-01,  1.5001e-01, -2.2395e-01,\n",
              "          2.1828e-01, -1.3919e-01,  1.5846e-01,  5.4114e-01, -5.6004e-01,\n",
              "          6.0383e-01,  1.1011e-01,  4.9327e-01, -4.6964e-01,  1.5001e-01,\n",
              "          7.1116e-01, -6.2504e-01, -4.5931e-01,  1.3420e-01,  5.6384e-01,\n",
              "         -5.7250e-01, -9.9786e-02, -3.4634e-01, -3.6623e-01,  5.6511e-02,\n",
              "          5.2688e-01,  1.2278e-01],\n",
              "        [ 5.0422e-01,  7.5871e-01,  5.3158e-01,  5.2986e-01, -3.0594e-01,\n",
              "          7.3102e-01, -1.8288e-02,  3.5977e-01, -2.3995e-01, -7.5178e-01,\n",
              "          4.4791e-01, -1.7065e-01, -2.9849e-01, -9.3218e-02,  2.5624e-01,\n",
              "          3.4057e-01, -4.9758e-01,  1.0680e-01, -4.8386e-01,  1.0558e+00,\n",
              "         -9.1445e-01,  6.0834e-02, -8.3554e-01, -1.1946e-01,  5.1192e-02,\n",
              "         -8.9272e-02,  1.0362e-01],\n",
              "        [-5.0464e-01, -7.8613e-01,  3.5075e-01, -8.2175e-01, -2.6931e-01,\n",
              "         -5.7837e-01,  9.7322e-01,  2.0727e-01,  6.9966e-01,  4.0190e-01,\n",
              "         -7.2755e-01,  1.8892e-01,  2.0150e-01, -1.1822e-01, -8.1386e-01,\n",
              "          1.5632e-01,  8.3550e-01,  2.9449e-01,  2.4464e-01, -2.4138e-01,\n",
              "          3.5047e-01,  2.2140e-01,  1.0918e+00, -1.0935e+00, -3.7177e-01,\n",
              "         -1.8837e-01, -1.1280e-01],\n",
              "        [ 3.7542e-01, -5.3345e-01, -2.7563e-01, -1.0231e+00,  4.6229e-01,\n",
              "         -1.1790e-01,  6.5366e-01, -9.2207e-03, -2.0723e-01,  1.0612e-01,\n",
              "         -1.4009e-01,  1.5780e-01,  3.6638e-02, -2.2783e-02,  8.5938e-02,\n",
              "          2.3483e-01,  8.4589e-01, -4.6583e-01,  8.6612e-02, -5.9724e-01,\n",
              "         -6.2930e-01, -3.7172e-01,  1.2837e+00, -6.1768e-01,  5.1300e-01,\n",
              "          2.5731e-03, -6.1152e-01],\n",
              "        [ 4.9584e-02,  8.5834e-01, -3.6744e-01,  4.5105e-01, -2.5290e-01,\n",
              "          8.3282e-01, -3.1379e-01, -1.3085e-01, -1.0667e-01, -2.1853e-01,\n",
              "          1.0287e+00, -6.4927e-01, -1.1408e-01, -1.0079e+00,  4.5079e-02,\n",
              "         -5.9540e-01, -5.7196e-01, -7.2266e-01, -9.6817e-01, -1.0861e-02,\n",
              "         -9.0215e-01, -2.9134e-01, -9.9745e-01,  1.5077e+00,  4.6537e-01,\n",
              "         -2.0287e-01,  4.1031e-01],\n",
              "        [ 7.9076e-01, -8.6827e-02, -7.0093e-01, -5.7057e-01, -1.4727e-02,\n",
              "          1.0672e+00,  1.9359e-01,  9.8943e-02, -8.6092e-02,  8.6421e-02,\n",
              "         -9.9097e-02,  5.2549e-01,  4.0166e-01,  5.6301e-02, -4.9597e-01,\n",
              "         -9.2773e-01,  1.2571e-01, -3.9993e-01,  6.1070e-01, -6.8704e-01,\n",
              "         -1.5508e-01, -3.3347e-01,  8.2663e-01,  5.1968e-01,  7.7319e-01,\n",
              "          3.8782e-01, -9.5242e-01],\n",
              "        [ 1.4038e-01, -3.7611e-01, -5.2203e-01, -1.3606e+00,  1.0061e-01,\n",
              "         -4.4039e-01,  8.2143e-01,  1.6820e-01, -1.7109e-01,  2.2337e-01,\n",
              "         -8.3546e-01,  2.6808e-01,  3.8318e-01, -2.3033e-01, -4.4771e-01,\n",
              "         -8.0779e-01,  4.4840e-01, -4.1604e-01, -1.9849e-01,  4.2585e-02,\n",
              "         -5.5691e-01, -7.9667e-03,  1.2847e+00, -7.1736e-01, -1.1624e-04,\n",
              "         -2.5981e-01, -1.2591e+00],\n",
              "        [-1.5736e-01, -2.0422e-01,  2.7241e-01, -5.3306e-01, -1.0975e-01,\n",
              "         -2.1122e-01,  1.1732e-01, -1.6795e-01,  1.9355e-01,  3.0351e-01,\n",
              "         -6.1219e-01, -1.6454e-01,  1.6302e-04,  1.9546e-02,  1.5540e-01,\n",
              "          5.5547e-01,  2.9951e-01,  2.6704e-01,  2.9015e-01, -6.3498e-01,\n",
              "          9.6106e-01, -8.2487e-01,  3.3751e-01, -1.1998e+00, -7.2470e-01,\n",
              "          7.9471e-02,  3.2751e-01],\n",
              "        [ 8.2299e-01,  2.8238e-01, -6.3434e-01, -4.2911e-01, -6.9005e-03,\n",
              "          7.7083e-01,  2.9631e-01,  2.6422e-01, -1.0493e+00, -2.6564e-01,\n",
              "          2.2404e-01, -2.2753e-02, -3.2471e-01, -4.1931e-01, -1.5096e-01,\n",
              "         -3.0943e-02,  9.4623e-01, -2.7775e-01,  6.7326e-01, -1.5712e-01,\n",
              "          9.7541e-03,  4.5144e-02,  3.6743e-01,  6.3235e-01,  9.3206e-01,\n",
              "         -8.9173e-01, -4.2298e-01],\n",
              "        [-3.6056e-01, -5.9388e-01, -5.2755e-02, -5.4272e-02,  3.0944e-01,\n",
              "         -2.3558e-01,  7.5751e-01,  2.1669e-02,  8.7665e-01, -1.2786e-01,\n",
              "         -2.3314e-01,  8.3527e-02, -9.9784e-02,  8.1662e-02, -9.1708e-02,\n",
              "          5.9108e-01, -3.5289e-02, -2.3347e-01,  1.3800e-01, -8.5046e-03,\n",
              "          2.4560e-01, -1.9107e-01,  6.0963e-01, -9.3596e-01, -2.5407e-02,\n",
              "          5.3047e-01,  1.8933e-01],\n",
              "        [ 3.5285e-01,  5.3474e-01, -1.2989e-01,  8.0402e-01,  3.5716e-01,\n",
              "          7.0059e-01,  2.8636e-01,  8.0572e-02,  2.8012e-03, -3.4715e-01,\n",
              "          1.4614e-01, -2.6103e-01, -4.9250e-01, -1.9617e-01, -9.6471e-01,\n",
              "          7.5746e-02,  2.5920e-01,  2.4569e-02,  4.9715e-01,  7.2528e-01,\n",
              "          6.6064e-01,  2.1069e-02, -2.9814e-01,  9.4049e-01,  1.0757e+00,\n",
              "         -2.3584e-01, -3.9803e-02],\n",
              "        [ 4.0151e-01, -2.6940e-01,  7.0311e-01, -6.5387e-01,  7.5129e-01,\n",
              "         -4.7694e-02, -7.4001e-01, -3.2602e-01, -6.8075e-01,  4.7014e-01,\n",
              "         -2.4869e-01, -4.4623e-02, -1.7615e-02,  8.6550e-01,  1.4311e-01,\n",
              "          1.0388e-01,  4.8082e-01,  2.1497e-01,  2.5033e-01, -8.6714e-01,\n",
              "          5.5024e-01, -1.1290e+00,  6.9154e-01, -3.4908e-01, -5.3839e-01,\n",
              "          2.3566e-01, -4.4463e-01]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eY1DQb5pCF4G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}